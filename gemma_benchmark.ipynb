{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma Model Comparison: Factual QA Benchmark\n",
        "\n",
        "**Author:** Kanishq Gandharv  \n",
        "**Date:** February 2026  \n",
        "**Purpose:** Reproducible benchmark comparing Google Gemma 2B and 7B models (base and instruction-tuned) on 100 factual questions\n",
        "\n",
        "## What This Notebook Does\n",
        "\n",
        "1. Loads 100 factual questions across 6 categories\n",
        "2. Evaluates 4 Gemma model variants:\n",
        "   - `google/gemma-2b` (base)\n",
        "   - `google/gemma-2b-it` (instruction-tuned)\n",
        "   - `google/gemma-7b` (base)\n",
        "   - `google/gemma-7b-it` (instruction-tuned)\n",
        "3. Saves detailed results and summary statistics\n",
        "4. Compares performance across models\n",
        "\n",
        "## Expected Results\n",
        "\n",
        "- **2B-IT**: ~85% accuracy (best performer)\n",
        "- **7B-IT**: ~69% accuracy\n",
        "- **Base models**: ~20% accuracy (not instruction-aligned)\n",
        "\n",
        "**Runtime:** ~45-60 minutes total on free Colab T4\n"
      ],
      "metadata": {
        "id": "GPPF8FgFH_uf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8UDU6u1O6En"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers accelerate torch bitsandbytes\n",
        "\n",
        "print(\"‚úì Dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup & Configuration\n",
        "\n",
        "Before running, you need:\n",
        "1. A Hugging Face account: https://huggingface.co/join\n",
        "2. Accept Gemma license: https://huggingface.co/google/gemma-2b\n",
        "3. Create access token: https://huggingface.co/settings/tokens (read permission)\n",
        "\n",
        "Replace `YOUR_TOKEN_HERE` below with your actual token.\n"
      ],
      "metadata": {
        "id": "IjGrFWjaIMVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# REQUIRED: Replace with your Hugging Face token\n",
        "HF_TOKEN = \"YOUR_TOKEN_HERE\"  # Get from https://huggingface.co/settings/tokens\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "\n",
        "print(\"‚úì Hugging Face token set successfully!\")\n",
        "print(\"‚ö†Ô∏è Make sure you've accepted the Gemma license at: https://huggingface.co/google/gemma-2b\")\n"
      ],
      "metadata": {
        "id": "BmOVO_JnQn4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Dataset\n",
        "\n",
        "100 factual questions across 6 balanced categories:\n",
        "- **Geography** (15): Capitals, continents, landmarks\n",
        "- **Math** (15): Arithmetic, geometry, basic algebra  \n",
        "- **Science** (20): Chemistry, physics, biology, astronomy\n",
        "- **History** (15): Major events, figures, dates\n",
        "- **Literature** (15): Authors and famous works\n",
        "- **General Knowledge** (20): Common facts, animals, everyday knowledge\n",
        "\n",
        "All questions have single, unambiguous answers suitable for substring matching.\n"
      ],
      "metadata": {
        "id": "2uLO4NF1IWDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 100-question evaluation dataset for Gemma Mini-Benchmark\n",
        "# Categories: Geography, Math, Science, History, Literature, General Knowledge\n",
        "\n",
        "eval_data = [\n",
        "    # Geography (15 questions)\n",
        "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
        "    {\"question\": \"What is the capital of Japan?\", \"answer\": \"Tokyo\"},\n",
        "    {\"question\": \"What is the capital of Italy?\", \"answer\": \"Rome\"},\n",
        "    {\"question\": \"What is the capital of Germany?\", \"answer\": \"Berlin\"},\n",
        "    {\"question\": \"What is the capital of Canada?\", \"answer\": \"Ottawa\"},\n",
        "    {\"question\": \"What is the capital of Australia?\", \"answer\": \"Canberra\"},\n",
        "    {\"question\": \"What is the capital of Brazil?\", \"answer\": \"Bras√≠lia\"},\n",
        "    {\"question\": \"What is the capital of India?\", \"answer\": \"New Delhi\"},\n",
        "    {\"question\": \"What is the capital of Russia?\", \"answer\": \"Moscow\"},\n",
        "    {\"question\": \"What is the capital of Egypt?\", \"answer\": \"Cairo\"},\n",
        "    {\"question\": \"What is the largest ocean on Earth?\", \"answer\": \"Pacific Ocean\"},\n",
        "    {\"question\": \"What is the smallest continent?\", \"answer\": \"Australia\"},\n",
        "    {\"question\": \"How many continents are there?\", \"answer\": \"7\"},\n",
        "    {\"question\": \"What is the longest river in the world?\", \"answer\": \"Nile\"},\n",
        "    {\"question\": \"What is the highest mountain in the world?\", \"answer\": \"Mount Everest\"},\n",
        "\n",
        "    # Math (15 questions)\n",
        "    {\"question\": \"What is 2 + 2?\", \"answer\": \"4\"},\n",
        "    {\"question\": \"What is 5 * 6?\", \"answer\": \"30\"},\n",
        "    {\"question\": \"What is 100 - 37?\", \"answer\": \"63\"},\n",
        "    {\"question\": \"What is 144 / 12?\", \"answer\": \"12\"},\n",
        "    {\"question\": \"What is the square root of 64?\", \"answer\": \"8\"},\n",
        "    {\"question\": \"What is 15% of 200?\", \"answer\": \"30\"},\n",
        "    {\"question\": \"What is the smallest prime number?\", \"answer\": \"2\"},\n",
        "    {\"question\": \"What is the next prime number after 7?\", \"answer\": \"11\"},\n",
        "    {\"question\": \"What is 2 to the power of 5?\", \"answer\": \"32\"},\n",
        "    {\"question\": \"What is the value of pi rounded to 2 decimal places?\", \"answer\": \"3.14\"},\n",
        "    {\"question\": \"How many sides does a hexagon have?\", \"answer\": \"6\"},\n",
        "    {\"question\": \"What is 1/4 as a decimal?\", \"answer\": \"0.25\"},\n",
        "    {\"question\": \"What is 20% of 50?\", \"answer\": \"10\"},\n",
        "    {\"question\": \"What is the cube of 3?\", \"answer\": \"27\"},\n",
        "    {\"question\": \"What is the sum of angles in a triangle?\", \"answer\": \"180\"},\n",
        "\n",
        "    # Science (20 questions)\n",
        "    {\"question\": \"What is the chemical symbol for water?\", \"answer\": \"H2O\"},\n",
        "    {\"question\": \"What is the chemical symbol for gold?\", \"answer\": \"Au\"},\n",
        "    {\"question\": \"What is the chemical symbol for oxygen?\", \"answer\": \"O\"},\n",
        "    {\"question\": \"What is the chemical symbol for carbon?\", \"answer\": \"C\"},\n",
        "    {\"question\": \"What is the speed of light in vacuum (in m/s)?\", \"answer\": \"299792458\"},\n",
        "    {\"question\": \"What is the largest planet in our solar system?\", \"answer\": \"Jupiter\"},\n",
        "    {\"question\": \"What is the smallest planet in our solar system?\", \"answer\": \"Mercury\"},\n",
        "    {\"question\": \"How many planets are in our solar system?\", \"answer\": \"8\"},\n",
        "    {\"question\": \"What is the closest planet to the Sun?\", \"answer\": \"Mercury\"},\n",
        "    {\"question\": \"What gas do plants absorb from the atmosphere?\", \"answer\": \"Carbon dioxide\"},\n",
        "    {\"question\": \"What gas do plants release during photosynthesis?\", \"answer\": \"Oxygen\"},\n",
        "    {\"question\": \"What is the powerhouse of the cell?\", \"answer\": \"Mitochondria\"},\n",
        "    {\"question\": \"What is DNA an abbreviation for?\", \"answer\": \"Deoxyribonucleic acid\"},\n",
        "    {\"question\": \"What is the hardest natural substance on Earth?\", \"answer\": \"Diamond\"},\n",
        "    {\"question\": \"What is the boiling point of water at sea level in Celsius?\", \"answer\": \"100\"},\n",
        "    {\"question\": \"What is the freezing point of water in Celsius?\", \"answer\": \"0\"},\n",
        "    {\"question\": \"How many bones are in the adult human body?\", \"answer\": \"206\"},\n",
        "    {\"question\": \"What is the largest organ in the human body?\", \"answer\": \"Skin\"},\n",
        "    {\"question\": \"What force keeps us on the ground?\", \"answer\": \"Gravity\"},\n",
        "    {\"question\": \"What is the atomic number of hydrogen?\", \"answer\": \"1\"},\n",
        "\n",
        "    # History (15 questions)\n",
        "    {\"question\": \"What year did World War 2 end?\", \"answer\": \"1945\"},\n",
        "    {\"question\": \"What year did World War 1 start?\", \"answer\": \"1914\"},\n",
        "    {\"question\": \"Who was the first President of the United States?\", \"answer\": \"George Washington\"},\n",
        "    {\"question\": \"In what year did the Titanic sink?\", \"answer\": \"1912\"},\n",
        "    {\"question\": \"Who discovered America in 1492?\", \"answer\": \"Christopher Columbus\"},\n",
        "    {\"question\": \"What year did the Berlin Wall fall?\", \"answer\": \"1989\"},\n",
        "    {\"question\": \"What year did India gain independence?\", \"answer\": \"1947\"},\n",
        "    {\"question\": \"Who was the first man on the moon?\", \"answer\": \"Neil Armstrong\"},\n",
        "    {\"question\": \"What year did humans first land on the moon?\", \"answer\": \"1969\"},\n",
        "    {\"question\": \"What ancient wonder is located in Egypt?\", \"answer\": \"Pyramids\"},\n",
        "    {\"question\": \"What year did the French Revolution begin?\", \"answer\": \"1789\"},\n",
        "    {\"question\": \"Who invented the telephone?\", \"answer\": \"Alexander Graham Bell\"},\n",
        "    {\"question\": \"Who invented the light bulb?\", \"answer\": \"Thomas Edison\"},\n",
        "    {\"question\": \"What year did the American Civil War end?\", \"answer\": \"1865\"},\n",
        "    {\"question\": \"What ancient civilization built Machu Picchu?\", \"answer\": \"Inca\"},\n",
        "\n",
        "    # Literature (15 questions)\n",
        "    {\"question\": \"Who wrote Romeo and Juliet?\", \"answer\": \"Shakespeare\"},\n",
        "    {\"question\": \"Who wrote Hamlet?\", \"answer\": \"Shakespeare\"},\n",
        "    {\"question\": \"Who wrote 1984?\", \"answer\": \"George Orwell\"},\n",
        "    {\"question\": \"Who wrote Pride and Prejudice?\", \"answer\": \"Jane Austen\"},\n",
        "    {\"question\": \"Who wrote The Great Gatsby?\", \"answer\": \"F. Scott Fitzgerald\"},\n",
        "    {\"question\": \"Who wrote Moby Dick?\", \"answer\": \"Herman Melville\"},\n",
        "    {\"question\": \"Who wrote To Kill a Mockingbird?\", \"answer\": \"Harper Lee\"},\n",
        "    {\"question\": \"Who wrote Harry Potter?\", \"answer\": \"J.K. Rowling\"},\n",
        "    {\"question\": \"Who wrote The Odyssey?\", \"answer\": \"Homer\"},\n",
        "    {\"question\": \"Who wrote The Iliad?\", \"answer\": \"Homer\"},\n",
        "    {\"question\": \"Who wrote Macbeth?\", \"answer\": \"Shakespeare\"},\n",
        "    {\"question\": \"Who wrote The Divine Comedy?\", \"answer\": \"Dante\"},\n",
        "    {\"question\": \"Who wrote War and Peace?\", \"answer\": \"Leo Tolstoy\"},\n",
        "    {\"question\": \"Who wrote Crime and Punishment?\", \"answer\": \"Fyodor Dostoevsky\"},\n",
        "    {\"question\": \"Who wrote The Lord of the Rings?\", \"answer\": \"J.R.R. Tolkien\"},\n",
        "\n",
        "    # General Knowledge (20 questions)\n",
        "    {\"question\": \"How many days are in a leap year?\", \"answer\": \"366\"},\n",
        "    {\"question\": \"How many hours are in a day?\", \"answer\": \"24\"},\n",
        "    {\"question\": \"How many minutes are in an hour?\", \"answer\": \"60\"},\n",
        "    {\"question\": \"How many seconds are in a minute?\", \"answer\": \"60\"},\n",
        "    {\"question\": \"How many weeks are in a year?\", \"answer\": \"52\"},\n",
        "    {\"question\": \"How many months are in a year?\", \"answer\": \"12\"},\n",
        "    {\"question\": \"What color is the sky on a clear day?\", \"answer\": \"Blue\"},\n",
        "    {\"question\": \"What is the opposite of hot?\", \"answer\": \"Cold\"},\n",
        "    {\"question\": \"What animal is known as man's best friend?\", \"answer\": \"Dog\"},\n",
        "    {\"question\": \"What is the largest land animal?\", \"answer\": \"Elephant\"},\n",
        "    {\"question\": \"What is the fastest land animal?\", \"answer\": \"Cheetah\"},\n",
        "    {\"question\": \"What bird is known for its ability to mimic human speech?\", \"answer\": \"Parrot\"},\n",
        "    {\"question\": \"How many legs does a spider have?\", \"answer\": \"8\"},\n",
        "    {\"question\": \"How many legs does an insect have?\", \"answer\": \"6\"},\n",
        "    {\"question\": \"What is the largest mammal in the world?\", \"answer\": \"Blue whale\"},\n",
        "    {\"question\": \"What is the tallest animal in the world?\", \"answer\": \"Giraffe\"},\n",
        "    {\"question\": \"What do bees produce?\", \"answer\": \"Honey\"},\n",
        "    {\"question\": \"What is the name of the fairy tale character who left a glass slipper?\", \"answer\": \"Cinderella\"},\n",
        "    {\"question\": \"What fruit is associated with keeping doctors away?\", \"answer\": \"Apple\"},\n",
        "    {\"question\": \"What vegetable makes you cry when you cut it?\", \"answer\": \"Onion\"}\n",
        "]\n",
        "\n",
        "print(f\"‚úì Loaded {len(eval_data)} evaluation questions\")\n",
        "print(f\"\\nBreakdown:\")\n",
        "print(f\"  Geography: 15\")\n",
        "print(f\"  Math: 15\")\n",
        "print(f\"  Science: 20\")\n",
        "print(f\"  History: 15\")\n",
        "print(f\"  Literature: 15\")\n",
        "print(f\"  General Knowledge: 20\")\n"
      ],
      "metadata": {
        "id": "VxwTJIwiPlht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions\n",
        "\n",
        "Define reusable functions for:\n",
        "- **Prompt formatting**: Converts questions into instruction-style prompts\n",
        "- **Answer generation**: Runs model inference with consistent settings\n",
        "- **String normalization**: Prepares text for case-insensitive matching\n",
        "- **Model loading**: Handles authentication and device placement\n",
        "- **Evaluation**: Runs full benchmark and saves results\n"
      ],
      "metadata": {
        "id": "-tEC0XGAIbXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import json\n",
        "\n",
        "def format_prompt(question: str) -> str:\n",
        "    \"\"\"Format question as instruction-style prompt.\"\"\"\n",
        "    return (\n",
        "        \"You are a helpful assistant. \"\n",
        "        \"Answer the following question in one short phrase.\\n\\n\"\n",
        "        f\"Question: {question}\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "\n",
        "def generate_answer(model, tokenizer, question, max_new_tokens=16):\n",
        "    \"\"\"Generate answer from Gemma model with consistent settings.\"\"\"\n",
        "    prompt = format_prompt(question)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,  # Deterministic greedy decoding\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only text after \"Answer:\" if present\n",
        "    if \"Answer:\" in full_text:\n",
        "        full_text = full_text.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "    return full_text\n",
        "\n",
        "def normalize(s: str) -> str:\n",
        "    \"\"\"Normalize string for case-insensitive matching.\"\"\"\n",
        "    return \"\".join(s.lower().split())\n",
        "\n",
        "def load_model(model_name: str):\n",
        "    \"\"\"Load Gemma model and tokenizer from Hugging Face.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Loading {model_name}...\")\n",
        "    print('='*60)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        token=HF_TOKEN,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "    print(f\"‚úì Model loaded successfully!\\n\")\n",
        "    return model, tokenizer\n",
        "\n",
        "def evaluate_model(model, tokenizer, model_name, eval_data, label_prefix):\n",
        "    \"\"\"Run full evaluation and save results.\"\"\"\n",
        "    correct = 0\n",
        "    results = []\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Evaluating {model_name} ({label_prefix})\")\n",
        "    print('='*60 + \"\\n\")\n",
        "\n",
        "    for i, item in enumerate(eval_data, 1):\n",
        "        question = item[\"question\"]\n",
        "        gold_answer = item[\"answer\"]\n",
        "\n",
        "        pred = generate_answer(model, tokenizer, question)\n",
        "        is_correct = normalize(gold_answer) in normalize(pred)\n",
        "        correct += int(is_correct)\n",
        "\n",
        "        result = {\n",
        "            \"question\": question,\n",
        "            \"gold_answer\": gold_answer,\n",
        "            \"predicted\": pred,\n",
        "            \"correct\": is_correct,\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        # Progress updates every 10 questions\n",
        "        if i % 10 == 0 or i == len(eval_data):\n",
        "            print(f\"[{i}/{len(eval_data)}] Progress: {correct}/{i} correct so far\")\n",
        "\n",
        "    accuracy = correct / len(eval_data)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"FINAL RESULTS: {model_name}\")\n",
        "    print('='*60)\n",
        "    print(f\"Accuracy: {accuracy:.2%} ({correct}/{len(eval_data)} correct)\")\n",
        "    print('='*60 + \"\\n\")\n",
        "\n",
        "    # Save detailed results (one JSON object per line)\n",
        "    results_path = f\"{label_prefix}_eval_results.jsonl\"\n",
        "    with open(results_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in results:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    # Save summary statistics\n",
        "    summary = {\n",
        "        \"model\": model_name,\n",
        "        \"label\": label_prefix,\n",
        "        \"total_questions\": len(eval_data),\n",
        "        \"correct\": correct,\n",
        "        \"accuracy\": accuracy,\n",
        "    }\n",
        "    summary_path = f\"{label_prefix}_eval_summary.json\"\n",
        "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(f\"Results saved to:\")\n",
        "    print(f\"  ‚Ä¢ {results_path} (detailed per-question results)\")\n",
        "    print(f\"  ‚Ä¢ {summary_path} (summary statistics)\\n\")\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "print(\"‚úì All helper functions defined successfully!\")\n"
      ],
      "metadata": {
        "id": "k4A33I3CPb2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation\n",
        "\n",
        "Running evaluation on all 4 models:\n",
        "\n",
        "### Estimated Runtime (Free Colab T4)\n",
        "- **gemma-2b**: ~5 minutes\n",
        "- **gemma-2b-it**: ~5 minutes  \n",
        "- **gemma-7b**: ~15 minutes\n",
        "- **gemma-7b-it**: ~15 minutes (with memory optimization)\n",
        "\n",
        "**Total: ~40-50 minutes**\n",
        "\n",
        "Each model processes 100 questions with greedy decoding. Results are saved incrementally, so you can check Files panel (left sidebar) for outputs as models complete.\n"
      ],
      "metadata": {
        "id": "6xmS2NuHIknY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define first 3 models to evaluate (skip 7B-IT for now)\n",
        "models_to_evaluate = [\n",
        "    (\"google/gemma-2b\",    \"gemma2b_base\"),\n",
        "    (\"google/gemma-2b-it\", \"gemma2b_it\"),\n",
        "    (\"google/gemma-7b\",    \"gemma7b_base\"),\n",
        "]\n",
        "\n",
        "# Store results\n",
        "all_accuracies = {}\n",
        "\n",
        "# Evaluate each model sequentially\n",
        "for model_name, label in models_to_evaluate:\n",
        "    model, tokenizer = load_model(model_name)\n",
        "    accuracy = evaluate_model(model, tokenizer, model_name, eval_data, label)\n",
        "    all_accuracies[label] = accuracy\n",
        "\n",
        "    # Free memory before loading next model\n",
        "    del model\n",
        "    del tokenizer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úì 2B and 7B base models complete!\")\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "id": "cibwQpfVPmbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating google/gemma-7b-it\n",
        "\n",
        "**Memory Challenge:** The 7B instruction-tuned model requires aggressive memory optimization on free Colab.\n",
        "\n",
        "**Optimizations applied:**\n",
        "- Reduced `max_new_tokens` from 16 ‚Üí 8\n",
        "- Explicit memory cleanup after each generation\n",
        "- Truncation of long input prompts\n",
        "- Mixed precision inference\n",
        "\n",
        "This may slightly impact accuracy compared to 16-token budget, but ensures the evaluation completes without OOM crashes.\n"
      ],
      "metadata": {
        "id": "I6Nle58IIvig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "# Hard memory reset\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
        "\n",
        "print(\"Loading google/gemma-7b-it (fp16, memory-optimized)...\")\n",
        "\n",
        "tokenizer_7b_it = AutoTokenizer.from_pretrained(\n",
        "    \"google/gemma-7b-it\",\n",
        "    token=HF_TOKEN,\n",
        ")\n",
        "\n",
        "model_7b_it = AutoModelForCausalLM.from_pretrained(\n",
        "    \"google/gemma-7b-it\",\n",
        "    token=HF_TOKEN,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    max_memory={0: \"13GB\"},\n",
        ")\n",
        "\n",
        "print(\"‚úì Model loaded!\\n\")\n",
        "\n",
        "def generate_answer_7b_it(question, max_new_tokens=8):\n",
        "    \"\"\"Memory-optimized generation for 7B-IT.\"\"\"\n",
        "    prompt = format_prompt(question)\n",
        "    inputs = tokenizer_7b_it(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model_7b_it.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer_7b_it.eos_token_id,\n",
        "                use_cache=True,\n",
        "            )\n",
        "\n",
        "    text = tokenizer_7b_it.decode(outputs[0], skip_special_tokens=True)\n",
        "    if \"Answer:\" in text:\n",
        "        text = text.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "    # Aggressive cleanup\n",
        "    del inputs, outputs\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Run evaluation\n",
        "correct = 0\n",
        "results = []\n",
        "\n",
        "print(\"Evaluating gemma-7b-it (fp16, 100 questions)...\\n\")\n",
        "\n",
        "for i, item in enumerate(eval_data, 1):\n",
        "    question = item[\"question\"]\n",
        "    gold_answer = item[\"answer\"]\n",
        "\n",
        "    try:\n",
        "        pred = generate_answer_7b_it(question)\n",
        "        is_correct = gold_answer.lower() in pred.lower()\n",
        "        correct += int(is_correct)\n",
        "\n",
        "        results.append({\n",
        "            \"question\": question,\n",
        "            \"gold_answer\": gold_answer,\n",
        "            \"predicted\": pred,\n",
        "            \"correct\": is_correct,\n",
        "        })\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"[{i}/100] {correct}/{i} correct\")\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(f\"\\n‚ö†Ô∏è OOM at question {i}\")\n",
        "        break\n",
        "\n",
        "accuracy = correct / len(results) if results else 0\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"Final: {accuracy:.2%} ({correct}/{len(results)})\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Save results\n",
        "with open(\"gemma7b_it_fp16_eval_results.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for r in results:\n",
        "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "summary = {\n",
        "    \"model\": \"google/gemma-7b-it\",\n",
        "    \"label\": \"gemma7b_it_fp16\",\n",
        "    \"quantization\": \"none (fp16)\",\n",
        "    \"total_questions\": len(results),\n",
        "    \"correct\": correct,\n",
        "    \"accuracy\": accuracy,\n",
        "}\n",
        "with open(\"gemma7b_it_fp16_eval_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"\\n‚úì Results saved!\")\n",
        "all_accuracies[\"gemma7b_it_fp16\"] = accuracy\n",
        "\n",
        "# Clean up\n",
        "del model_7b_it, tokenizer_7b_it\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "WETroe6ASS2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Results & Comparison\n",
        "\n",
        "All 4 models evaluated. Loading results from saved summary files and displaying comparison table.\n"
      ],
      "metadata": {
        "id": "qJx1OYxMI35E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL COMPARISON - 100 Question Factual QA\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Load accuracies from saved summary files\n",
        "summary_files = {\n",
        "    \"gemma2b_base\": \"gemma2b_base_eval_summary.json\",\n",
        "    \"gemma2b_it\": \"gemma2b_it_eval_summary.json\",\n",
        "    \"gemma7b_base\": \"gemma7b_base_eval_summary.json\",\n",
        "    \"gemma7b_it_fp16\": \"gemma7b_it_fp16_eval_summary.json\",\n",
        "}\n",
        "\n",
        "accuracies = {}\n",
        "for label, filename in summary_files.items():\n",
        "    try:\n",
        "        with open(filename, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "            accuracies[label] = data[\"accuracy\"]\n",
        "    except FileNotFoundError:\n",
        "        accuracies[label] = None\n",
        "\n",
        "# Print comparison table\n",
        "print(f\"{'Model':<25} {'Params':<10} {'Type':<20} {'Accuracy':<10}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "model_info = [\n",
        "    (\"google/gemma-2b\",    \"gemma2b_base\",    \"2B\", \"base\"),\n",
        "    (\"google/gemma-2b-it\", \"gemma2b_it\",      \"2B\", \"instruct\"),\n",
        "    (\"google/gemma-7b\",    \"gemma7b_base\",    \"7B\", \"base\"),\n",
        "    (\"google/gemma-7b-it\", \"gemma7b_it_fp16\", \"7B\", \"instruct (fp16)\"),\n",
        "]\n",
        "\n",
        "for model_name, label, params, model_type in model_info:\n",
        "    acc = accuracies.get(label)\n",
        "    if acc is not None:\n",
        "        print(f\"{model_name:<25} {params:<10} {model_type:<20} {acc:.2%}\")\n",
        "    else:\n",
        "        print(f\"{model_name:<25} {params:<10} {model_type:<20} N/A\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"All result files saved in Files panel (left sidebar)\")\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "id": "zjQKGh2Eb0H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Findings\n",
        "\n",
        "### 1. Instruction Tuning is Critical\n",
        "- **2B-IT vs 2B Base**: +66 percentage points (19% ‚Üí 85%)\n",
        "- Base models generate continuations instead of direct answers\n",
        "\n",
        "### 2. Surprising Result: 2B-IT Outperforms 7B-IT\n",
        "- **2B-IT: 85%** vs **7B-IT: 69%**\n",
        "- Likely due to memory constraints (8 tokens vs 16)\n",
        "- Future work: Re-evaluate 7B-IT with full 16-token budget\n",
        "\n",
        "### 3. Scale Without Tuning Adds Little Value\n",
        "- 7B base only +2% over 2B base (21% vs 19%)\n",
        "- Instruction alignment > raw parameter count\n",
        "\n",
        "## Download Results\n",
        "\n",
        "Click the Files icon (üìÅ) on the left sidebar to download:\n",
        "- `*_eval_results.jsonl` - Per-question detailed results\n",
        "- `*_eval_summary.json` - Accuracy statistics\n",
        "\n",
        "Upload these to your GitHub repo under `results/` folder.\n"
      ],
      "metadata": {
        "id": "eMUVEm0PJBCn"
      }
    }
  ]
}